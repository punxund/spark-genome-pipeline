#!/usr/bin/env python3
"""
Spark ìœ ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” íŒŒì´í”„ë¼ì¸ì˜ ê° ëª¨ë“ˆì„ ê°œë³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
"""

import sys
import logging
from pathlib import Path
import argparse

from pyspark.sql import SparkSession

# í”„ë¡œì íŠ¸ ëª¨ë“ˆë“¤ import
from config import Config
from utils import check_tool_availability, parse_fastq_pairs
from preprocessing import FastQPreprocessor
from alignment import BWAAligner
from sam_processing import SAMProcessor
from coverage import CoverageCalculator

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def create_test_spark_session() -> SparkSession:
    """í…ŒìŠ¤íŠ¸ìš© Spark ì„¸ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    spark = SparkSession.builder \
        .appName("GenomePipelineTest") \
        .master("local[*]") \
        .config("spark.driver.memory", "2g") \
        .config("spark.executor.memory", "2g") \
        .getOrCreate()
    
    logger.info(f"í…ŒìŠ¤íŠ¸ Spark ì„¸ì…˜ ìƒì„±: {spark.sparkContext.applicationId}")
    return spark

def test_dependencies():
    """ì˜ì¡´ì„± ë„êµ¬ë“¤ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    logger.info("=" * 50)
    logger.info("ì˜ì¡´ì„± ë„êµ¬ í…ŒìŠ¤íŠ¸")
    logger.info("=" * 50)
    
    tools = {
        "fastp": Config.FASTP_PATH,
        "bwa": Config.BWA_PATH,
        "samtools": Config.SAMTOOLS_PATH,
        "bedtools": Config.BEDTOOLS_PATH,
        "bedGraphToBigWig": Config.BIGWIG_PATH
    }
    
    all_available = True
    for tool_name, tool_path in tools.items():
        if check_tool_availability(tool_name, tool_path):
            logger.info(f"âœ“ {tool_name}: ì‚¬ìš© ê°€ëŠ¥")
        else:
            logger.warning(f"âœ— {tool_name}: ì‚¬ìš© ë¶ˆê°€")
            all_available = False
    
    if all_available:
        logger.info("ëª¨ë“  ì˜ì¡´ì„± ë„êµ¬ê°€ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤!")
    else:
        logger.warning("ì¼ë¶€ ë„êµ¬ê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
    
    return all_available

def test_input_files():
    """ì…ë ¥ íŒŒì¼ë“¤ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    logger.info("=" * 50)
    logger.info("ì…ë ¥ íŒŒì¼ í…ŒìŠ¤íŠ¸")
    logger.info("=" * 50)
    
    # ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸
    if not Config.DATA_DIR.exists():
        logger.error(f"ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {Config.DATA_DIR}")
        return False
    
    # ì½ê¸° íŒŒì¼ í™•ì¸
    if not Config.READS_DIR.exists():
        logger.error(f"ì½ê¸° íŒŒì¼ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {Config.READS_DIR}")
        return False
    
    # ì°¸ì¡° ê²Œë†ˆ í™•ì¸
    if not Config.REFERENCE_GENOME.exists():
        logger.error(f"ì°¸ì¡° ê²Œë†ˆ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {Config.REFERENCE_GENOME}")
        return False
    
    if not Config.REFERENCE_INDEX.exists():
        logger.error(f"ì°¸ì¡° ì¸ë±ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {Config.REFERENCE_INDEX}")
        return False
    
    # FASTQ íŒŒì¼ ìŒ íŒŒì‹±
    try:
        fastq_pairs = parse_fastq_pairs(Config.READS_DIR)
        if fastq_pairs:
            logger.info(f"FASTQ íŒŒì¼ ìŒ {len(fastq_pairs)}ê°œ ë°œê²¬:")
            for sample_id, r1_file, r2_file in fastq_pairs:
                logger.info(f"  - {sample_id}: {r1_file.name}, {r2_file.name}")
        else:
            logger.warning("FASTQ íŒŒì¼ ìŒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
    except Exception as e:
        logger.error(f"FASTQ íŒŒì¼ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        return False
    
    logger.info("ëª¨ë“  ì…ë ¥ íŒŒì¼ì´ ì •ìƒì…ë‹ˆë‹¤!")
    return True

def test_preprocessing_module(spark: SparkSession):
    """ì „ì²˜ë¦¬ ëª¨ë“ˆì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    logger.info("=" * 50)
    logger.info("ì „ì²˜ë¦¬ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    logger.info("=" * 50)
    
    try:
        # ì „ì²˜ë¦¬ê¸° ìƒì„±
        preprocessor = FastQPreprocessor(spark)
        logger.info("âœ“ FastQPreprocessor ìƒì„± ì„±ê³µ")
        
        # FASTQ íŒŒì¼ ìŒ íŒŒì‹±
        fastq_pairs = parse_fastq_pairs(Config.READS_DIR)
        if not fastq_pairs:
            logger.error("í…ŒìŠ¤íŠ¸í•  FASTQ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # DataFrame ìƒì„± í…ŒìŠ¤íŠ¸
        df = preprocessor.create_fastq_dataframe(fastq_pairs)
        logger.info(f"âœ“ DataFrame ìƒì„± ì„±ê³µ: {df.count()}ê°œ í–‰")
        
        # ì²« ë²ˆì§¸ ìƒ˜í”Œë§Œ í…ŒìŠ¤íŠ¸
        test_sample = fastq_pairs[0]
        logger.info(f"í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {test_sample[0]}")
        
        # UDF í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ì‹¤í–‰ì€ í•˜ì§€ ì•ŠìŒ)
        logger.info("UDF ë“±ë¡ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
        preprocessor.cleanup()
        logger.info("âœ“ ì „ì²˜ë¦¬ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"ì „ì²˜ë¦¬ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_alignment_module(spark: SparkSession):
    """ì •ë ¬ ëª¨ë“ˆì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    logger.info("=" * 50)
    logger.info("ì •ë ¬ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    logger.info("=" * 50)
    
    try:
        # ì •ë ¬ê¸° ìƒì„±
        aligner = BWAAligner(spark, Config.REFERENCE_GENOME)
        logger.info("âœ“ BWAAligner ìƒì„± ì„±ê³µ")
        
        # ì°¸ì¡° ê²Œë†ˆ ì¸ë±ì‹± í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ì‹¤í–‰ì€ í•˜ì§€ ì•ŠìŒ)
        logger.info("ì°¸ì¡° ê²Œë†ˆ ì¸ë±ì‹± í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
        aligner.cleanup()
        logger.info("âœ“ ì •ë ¬ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"ì •ë ¬ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_sam_processing_module(spark: SparkSession):
    """SAM ì²˜ë¦¬ ëª¨ë“ˆì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    logger.info("=" * 50)
    logger.info("SAM ì²˜ë¦¬ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    logger.info("=" * 50)
    
    try:
        # SAM ì²˜ë¦¬ê¸° ìƒì„±
        processor = SAMProcessor(spark)
        logger.info("âœ“ SAMProcessor ìƒì„± ì„±ê³µ")
        
        # í†µê³„ íŒŒì‹± í…ŒìŠ¤íŠ¸
        test_flagstat = """1000 + 0 in total (QC-passed reads + QC-failed reads)
800 + 0 mapped (80.00% : N/A)
200 + 0 paired in sequencing
150 + 0 properly paired (75.00% : N/A)
50 + 0 singletons (25.00% : N/A)
0 + 0 with mate mapped to a different chr
0 + 0 with mate mapped to a different chr (mapQ>=5)"""
        
        stats = processor.parse_flagstat(test_flagstat)
        logger.info(f"âœ“ í†µê³„ íŒŒì‹± í…ŒìŠ¤íŠ¸ ì„±ê³µ: {stats}")
        
        processor.cleanup()
        logger.info("âœ“ SAM ì²˜ë¦¬ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"SAM ì²˜ë¦¬ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_coverage_module(spark: SparkSession):
    """ì»¤ë²„ë¦¬ì§€ ëª¨ë“ˆì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    logger.info("=" * 50)
    logger.info("ì»¤ë²„ë¦¬ì§€ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    logger.info("=" * 50)
    
    try:
        # ì»¤ë²„ë¦¬ì§€ ê³„ì‚°ê¸° ìƒì„±
        calculator = CoverageCalculator(spark, Config.REFERENCE_INDEX)
        logger.info("âœ“ CoverageCalculator ìƒì„± ì„±ê³µ")
        
        # í†µê³„ ê³„ì‚° í…ŒìŠ¤íŠ¸
        import pandas as pd
        test_data = {
            'chrom': ['chr1', 'chr1', 'chr1', 'chr2'],
            'start': [0, 100, 200, 0],
            'end': [100, 200, 300, 100],
            'coverage': [10, 25, 5, 15]
        }
        test_df = pd.DataFrame(test_data)
        
        # ì„ì‹œ íŒŒì¼ ìƒì„±
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', suffix='.bed', delete=False) as f:
            test_df.to_csv(f.name, sep='\t', header=False, index=False)
            stats = calculator.calculate_coverage_stats(Path(f.name))
        
        logger.info(f"âœ“ í†µê³„ ê³„ì‚° í…ŒìŠ¤íŠ¸ ì„±ê³µ: {stats}")
        
        calculator.cleanup()
        logger.info("âœ“ ì»¤ë²„ë¦¬ì§€ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"ì»¤ë²„ë¦¬ì§€ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_spark_functionality(spark: SparkSession):
    """Spark ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    logger.info("=" * 50)
    logger.info("Spark ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
    logger.info("=" * 50)
    
    try:
        # ê°„ë‹¨í•œ DataFrame ìƒì„± ë° ì—°ì‚° í…ŒìŠ¤íŠ¸
        data = [("sample1", "file1.fastq", "file2.fastq"),
                ("sample2", "file3.fastq", "file4.fastq")]
        
        df = spark.createDataFrame(data, ["sample_id", "r1_file", "r2_file"])
        logger.info(f"âœ“ DataFrame ìƒì„± ì„±ê³µ: {df.count()}ê°œ í–‰")
        
        # í•„í„°ë§ í…ŒìŠ¤íŠ¸
        filtered_df = df.filter(df.sample_id == "sample1")
        logger.info(f"âœ“ í•„í„°ë§ í…ŒìŠ¤íŠ¸ ì„±ê³µ: {filtered_df.count()}ê°œ í–‰")
        
        # UDF ë“±ë¡ í…ŒìŠ¤íŠ¸
        from pyspark.sql.functions import udf
        from pyspark.sql.types import StringType
        
        def test_udf(sample_id):
            return f"processed_{sample_id}"
        
        test_udf_func = udf(test_udf, StringType())
        result_df = df.withColumn("processed_id", test_udf_func(df.sample_id))
        logger.info("âœ“ UDF ë“±ë¡ ë° ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        
        logger.info("âœ“ Spark ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.error(f"Spark ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def run_all_tests():
    """ëª¨ë“  í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    logger.info("Spark ìœ ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # Spark ì„¸ì…˜ ìƒì„±
    spark = create_test_spark_session()
    
    test_results = {}
    
    try:
        # 1. ì˜ì¡´ì„± í…ŒìŠ¤íŠ¸
        test_results["dependencies"] = test_dependencies()
        
        # 2. ì…ë ¥ íŒŒì¼ í…ŒìŠ¤íŠ¸
        test_results["input_files"] = test_input_files()
        
        # 3. Spark ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
        test_results["spark_functionality"] = test_spark_functionality(spark)
        
        # 4. ëª¨ë“ˆë³„ í…ŒìŠ¤íŠ¸
        if test_results["dependencies"] and test_results["input_files"]:
            test_results["preprocessing"] = test_preprocessing_module(spark)
            test_results["alignment"] = test_alignment_module(spark)
            test_results["sam_processing"] = test_sam_processing_module(spark)
            test_results["coverage"] = test_coverage_module(spark)
        
        # ê²°ê³¼ ìš”ì•½
        logger.info("=" * 50)
        logger.info("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        logger.info("=" * 50)
        
        passed = 0
        total = len(test_results)
        
        for test_name, result in test_results.items():
            status = "âœ“ í†µê³¼" if result else "âœ— ì‹¤íŒ¨"
            logger.info(f"{test_name}: {status}")
            if result:
                passed += 1
        
        logger.info(f"ì „ì²´ ê²°ê³¼: {passed}/{total} í†µê³¼")
        
        if passed == total:
            logger.info("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ í†µê³¼í–ˆìŠµë‹ˆë‹¤!")
            return True
        else:
            logger.warning("âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return False
            
    finally:
        spark.stop()
        logger.info("Spark ì„¸ì…˜ ì¢…ë£Œ")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Spark ìœ ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸")
    parser.add_argument("--test", choices=["all", "dependencies", "input", "spark", "preprocessing", "alignment", "sam", "coverage"], 
                       default="all", help="ì‹¤í–‰í•  í…ŒìŠ¤íŠ¸ (ê¸°ë³¸ê°’: all)")
    
    args = parser.parse_args()
    
    if args.test == "all":
        success = run_all_tests()
        return 0 if success else 1
    else:
        logger.info(f"ê°œë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰: {args.test}")
        # ê°œë³„ í…ŒìŠ¤íŠ¸ êµ¬í˜„ì€ ìƒëµ (í•„ìš”ì‹œ ì¶”ê°€)
        return 0

if __name__ == "__main__":
    sys.exit(main())
